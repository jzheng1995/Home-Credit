---
title: "Preliminary Data Extraction"
author: "Jason"
output: 
  md_document:
    variant: markdown_github
---
## Introduction

This is primarily an ETL document for the 2024 home credit competition data on Kaggle. This will also be a first attempt at 1) a kaggle competition, 2) approaching this dataset, and 3) documenting the bottom-up process for public review. This will primarily be a reference for myself on how to improve on all these aspects. The goal of this competition is to predict `target` class from all the provided features. 

## Set up

### Libraries
```{r}
library(tidyverse)
library(here)
library(DBI)
library(RSQLite)
library(reticulate)
library(tools)
library(data.tree)
library(rmarkdown)

con <- dbConnect(SQLite(), "mydb")
```

```{r}
# quick rds save/load
rdsread <- function(name, envr = globalenv()){
  assign(x = name, read_rds(str_c(name,".rds")),envir = envr)
}
quickrds <- function(x){
  saveRDS(get(x),str_c(x, ".rds"))
}
```

### File import

When I first approached these data, I attempted to read the all the csv files into my R environment through `read_csv()`. As it turns out, the data contained millions of rows for multiple files and were clearly too big to import for my laptop memory. My roundabout strategy in circumventing this problem was creating a local relational database (SQLite), trim the data down on the database side, and then import a data table that's hopefully small enough to work with. 

To do all this, I planned on reading each csv data file into a local database. I'm not a fan of manually writing all my file names so first I set up all the folder paths to read into a local SQLite database. 

```{r class.source = 'fold-show'}
# folder paths
train_folder_path <- here("home-credit-credit-risk-model-stability","csv_files","train") %>% str_c(.,"/") 
test_folder_path <- here("home-credit-credit-risk-model-stability","csv_files","test") %>% str_c(.,"/") 

# file paths
train_names <- here("home-credit-credit-risk-model-stability","csv_files","train") %>% list.files()
test_names <- here("home-credit-credit-risk-model-stability","csv_files","test") %>% list.files()

# test/train full file paths
train_paths <- str_c(train_folder_path, train_names)
test_paths <- str_c(test_folder_path, test_names)

# Names for each data table
test_names_noext <- test_names %>% file_path_sans_ext()
train_names_noext <- train_names %>% file_path_sans_ext()

```

I then iterated the creation of a database table for each csv by running a `dbWriteTable()` for each filepath. 

```{r class.source = 'fold-show', eval = FALSE}
# Import all csv files into one SQL database

# for each csv file in train folder, read into mydb
for (i in seq(length(train_paths))){
  dbWriteTable(con, train_names_noext[i], train_paths[i], overwrite = TRUE)
}

# for each csv file in test folder, read into mydb
for (i in seq(length(test_paths))){
  dbWriteTable(con, test_names_noext[i], test_paths[i], overwrite = TRUE)
}
```

## Overview

### SQL overview

Since the data is relatively unknown at this point, here are some useful SQL/r functions to extract some useful overviews. We'll perform these on the `train` subset of csv files. 

* `dbListFields`: DBI function to examine column names
* `"PRAGMA table_info"`: SQL statement to examine column data type

```{r}
# table fields
train_fields <- tibble(table_name = train_names_noext) %>% mutate(
  fields = map(table_name, ~dbListFields(con, .)),
  info = map(table_name, ~dbGetQuery(con, str_c("PRAGMA table_info(",.,")")))
)
# show
train_fields %>% select(info) %>% unnest(info) %>% select(name, type) %>%  head()
```

* `dbListTables`: DBI function to examine existing table names

```{r}
# table names
datatables <- dbListTables(con)
# original table names
original_table_names <- tibble(table = datatables) %>%  
  # filter out future added table names
  filter(str_detect(table, "test|train"),
  str_detect(table, "id|ID|_U|joined", negate = TRUE)) 
# show
original_table_names %>% head() 
```

There are `r train_fields %>% unnest(info) %>% nrow()` columns across `r original_table_names %>% nrow()` tables. Some recurring columns are `case_id`, `num_group1`, and `num_group2`. `case_id` is clearly an unique row identifier for each person. Based on the Kaggle data description, `num_group1` and `num_group2` are meant to index `case_id` when there are multiple row entries per `case_id` (e.g. historical loan contracts). 

### File naming patterns

We can also examine the structure of the files with a `data.tree` below. 

```{r}
# set up nodes
homecredit<- Node$new("home-credit-credit-risk-model-stability")
csvfiles <- homecredit$AddChild("csv_files")
train <- csvfiles$AddChild("train")
test <- csvfiles$AddChild("test")

# add train nodes
for (i in (original_table_names %>% filter(str_detect(table, "train")) %>% pull(table))){
  train$AddChild(i)}
# add test nodes
for (i in (original_table_names %>% filter(str_detect(table, "test")) %>% pull(table))){
  test$AddChild(i)}
# show
homecredit %>% 
  as.data.frame() %>% 
  as.matrix() %>% 
  print(quote=FALSE)
```



We see that all the original csv files are organized by a prefix `test_` or `train_`. These can be further related by numeral suffixes that denote either 1) related tables split by columns (`train_person_1`, `train_person_2`) or 2) related tables split by rows (`train_credit_bureau_a_2_1`,`train_credit_bureau_a_2_2`,...,`train_credit_bureau_a_2_8`). 

In addition, a `feature_definitions.csv` file was provided that mapped column names to feature definitions. For future reference, we create a tibble to map feature definitions to column names to table names. 

```{r,class.output ="watch-out"}
# import feature definitions
features <- here("home-credit-credit-risk-model-stability","feature_definitions.csv") %>% read_csv(show_col_types = FALSE)

# table fields mapped to feature description
train_features <- train_fields %>% unnest(everything()) %>% left_join(features, by = c("fields" = "Variable")) %>% select(fields, everything())
train_features %>% select(fields, table_name,Description ) %>% head()
```

### Base_train

The goal of this competition is to predict `target`, which can be found in `train_base`. The definition of `target`, found on a [forum Q&A](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/477074), is defined as:  
```
...unpaid payment (one is enough) in certain time period. There is also some time tolerance (e.g. one day late is not default) and amount tolerance (if client paid $100 instead of $100.10) ). 
```

Here's some information on `train_base`.

* **Column names, type**

    ```{r}
# Base training dataset - summary
# table columns
base_columns <- dbGetQuery(con, "PRAGMA table_info(train_base)")
base_columns %>% head() 
    ```

* **Number of rows**

    ```{r}
# table rows
base_rows <- dbGetQuery(con, "SELECT COUNT(*) FROM train_base")
base_rows %>% head() 
    ```

* **Number of rows per participant**

    ```{r}
# rows per participant
base_rows_per_id <- dbGetQuery(con, "SELECT case_id, COUNT(*) FROM train_base GROUP BY case_id")
base_rows_per_id %>% head()
# sum of rows
row_per_users <- dbGetQuery(con, "SELECT row_count, count(*) as user_count
FROM 
(SELECT count(case_id) AS row_count 
FROM train_base
GROUP BY case_id)
GROUP BY row_count")

row_per_users %>% head() 
    ```

Now we have an idea of the main table we'll be working with. Since the prediction `target` is only available for the `case_id` present in `train_base`, the first step for reducing the data load is to filter for the same `case_id` in all the other supplemental tables. Before we do that, however, I want to simplify the number of tables I have by joining tables split by rows. This will reduce the number of table `LEFT JOIN`s I will have to perform later on. 

### Joining split tables

The split tables we're looking for share the same `columns` but different `rows`. We want to 1) identify tables sharing the same columns , 2) identify the shared naming scheme that tbese split tables use, and 3) join the split table rows by the common naming scheme.  Here are the steps to set this up:

* Create a tibble with original table names mapped to all column names

```{r,class.source = 'fold-show'}
# renew datatable
datatables <- dbListTables(con)
# Filter original tables
original_fields <- tibble(table = datatables) %>% filter(
  str_detect(table, "test|train"),
  str_detect(table, "id|ID|_U|joined", negate = TRUE)
) %>% mutate(
  fields = map(table, ~dbListFields(con, .)),
  info = map(table, ~dbGetQuery(con, str_c("PRAGMA table_info(",.,")")))
) %>% select(table, fields) %>% unnest(fields)
```


* First extract common naming scheme using regex. We then map the naming scheme to original table name and its respective number of columns. We expect that for any set tables sharing a common scheme, the set should share the same number of columns.

```{r,class.source = 'fold-show'}
# map extracted scheme to original table name, # of columns
split_datasets_revised <- original_fields %>% select(table) %>% unique() %>% 
  mutate(table_name_extract =str_extract(table,"[A-Za-z_]*_\\d(?=_\\d)" )) %>% 
  select(table, table_name_extract) %>% mutate(
  ncol = map(table, ~dbGetQuery(con, str_c("PRAGMA table_info(",.,")")) %>% nrow()) ) %>% unnest(ncol)
split_datasets_revised %>% head()
```

* Set up the join query for split tables. Since these are vertical rowbinding, we use `UNION` statements to create new joined tables suffxied with `_U`. 

```{r}
# joined table set names
split_dataset_names <- split_datasets_revised$table_name_extract %>% unique() %>% discard(is.na)
# create UNION/create table query for every split tables
union_query_revised <- 
  # extract unique shared table names
  split_datasets_revised %>% select(table_name_extract) %>% na.omit() %>% unique() %>% 
  # join to identify split tables
  left_join(split_datasets_revised,by = join_by(table_name_extract)) %>%
  # add # of split tables per group
  group_by(table_name_extract) %>% add_tally() %>% 
  # iterate SELECT statements for SQL query
  mutate(queries = str_c("SELECT * FROM ", table)) %>% 
  # create single query for each table
  select(table_name_extract, queries) %>% group_by(table_name_extract) %>% nest(nested_query = queries) %>% 
  # create combined table
  mutate(
         combined_query = nested_query[[1]] %>% pull(queries) %>% paste0(., collapse = " "),
         union_query = str_replace_all(combined_query,"(\\d )(?=SELECT)","\\1UNION " ),
         write_query = str_c("CREATE TABLE IF NOT EXISTS ",table_name_extract,"_U"," AS ", union_query))

# examine the queries
union_query_revised %>% select(table_name_extract,write_query) %>% head() 
quickrds("split_dataset_names")

```

```{r, eval = FALSE}
# SQL - union 
# execute each query
union_query_revised$write_query %>% walk(~dbExecute(con,.))
```

## Subset supplemental data

Since the main focus of the operation is predicting `target` from `train_base`, we can reduce our operations to just data that's available for `case_id`s that are present in `train_base`. This can be done by using `INNER JOIN` and filtering out data from all supplemental sources with `train_base$case_id`. 

* First subset `case_id` column from base tables
```{r, eval = FALSE}
# create IDs to filter out relevant IDs in data tables
dbExecute(con,"CREATE TABLE IF NOT EXISTS train_id AS SELECT case_id FROM train_base")
dbExecute(con,"CREATE TABLE IF NOT EXISTS test_id AS SELECT case_id FROM test_base")
```

* Some of supplemental tables have multiple rows per participant. The key for the main participant data is the value 0 for numgroup_1 and numgroup_2. Here we set up a numgroup tibble for future mapping. (1 = 0 mapping, 0 = other number)
```{r}
# numgroup index
numgroup_tibble <- original_fields %>% 
  filter(str_detect(fields, "num_group")) %>% 
  pivot_wider(names_from = fields, values_from = fields, values_fn = ~1, values_fill = 0) 

numgroup_tibble %>% head()
```

* Create query to `INNER JOIN` and filter tables by `train_base$case_id`, `numgroup` and save filtered tables with a `_id` suffix. 
    ```{r}
  # renew data table
datatables <- dbListTables(con)

  # filter data by id
id_join_query_revised <- tibble(tables = datatables) %>% 
  # filter for original tables,joined tables 
  filter(
    # filter out split table names
    str_detect(tables,str_c(str_c(split_dataset_names,"_\\d"), collapse = "|"),
               negate = TRUE))%>% 
  filter(str_detect(tables, "id|ID|joined|base", negate = TRUE)) %>% 
  filter(str_detect(tables, "train|test")) %>% 
  # create query
  mutate(
  query = case_when(
    str_detect(tables, "train") ~ str_c("CREATE TABLE IF NOT EXISTS ",tables, "_id"," AS SELECT * FROM ",tables,
                " INNER JOIN train_id ON ", tables),
    str_detect(tables, "test") ~ str_c("CREATE TABLE IF NOT EXISTS ",tables, "_id"," AS SELECT * FROM ",tables,
                " INNER JOIN test_id ON ", tables)
  ),
  query = case_when(
    str_detect(tables, "train") ~ str_c(query, ".case_id = train_id.case_id"),
    str_detect(tables, "test") ~ str_c(query, ".case_id = test_id.case_id")
  )
)
  # filter rows by num_group
id_join_query_num <- id_join_query_revised %>% left_join(numgroup_tibble , by = c("tables" = "table")) %>% mutate(
  query = case_when(
    num_group1 == 1 & num_group2 == 0 ~ str_c(query, " AND num_group1 = 0"),
    num_group1 == 1 & num_group2 == 1  ~ str_c(query, " AND num_group1 = 0 AND num_group2 = 0"),
    .default = query
  )
)
  # show queries
id_join_query_num$query %>% head()
    ```

```{r, eval = FALSE}
# execute
id_join_query_num$query %>% walk(~dbExecute(con,.))
```

## Joining features 

### Feature subset
To facilitate a quick analysis we're going to pick out some variables that I think are theoretically relevant in predicting `target`:

1. `birth_259D`: Birth date
1. `education_927M`: Education
1. `empl_employedfrom_271D`: Employment date
1. `empl_employedtotal_800L`: Employment length
1. `empl_industry_691L`: Job industry
1. `familystate_447L`: Family status
1. `incometype_1044TL`: Salary type
1. `mainoccupationinc_384A`: Income amount
1. `riskassesment_940T`: Normalized risk - assessed by credit bureau

### Join SQL

* We can use a tibble to map the selected features to their respective tables. 

    ```{r}
# highlight features of interest and identify within datatables

# pick features of interest and rename
revised_feature_list <- list(
  base = c("case_id"),
  person_1_id = c("birth_259D", #birthdate
               "education_927M", # education
               "empl_employedfrom_271D", # employ date
               "empl_employedtotal_800L", # employ length
               "empl_industry_691L", # job industry
               "familystate_447L", # relationship
               "incometype_1044T", # salary type
               "mainoccupationinc_384A"# income amount
               ), 
  static_cb_0_id = c("riskassesment_940T" #risk assessment - normalized
                           )) 
# link features of interest to data tables
revised_features_tbl <- 
  # set up variables as tibble
  tibble(table = names(revised_feature_list),
         train_table = str_c("train_", table),
         test_table = str_c("test_", table),
       features = revised_feature_list ) %>% unnest(features)
    ```

* We'll then set up the query parts using the mapped tibble.
    ```{r}
# create query for joining relevant features to train_base
train_join <- revised_features_tbl %>% filter(str_detect(features, "case_id", negate = TRUE)) %>% na.omit() %>% 
  # table.column query
  mutate(
  table_column = str_c(train_table,".", features)) %>% 
  select(train_table, table_column) %>% 
  mutate(
    join_string = str_c("LEFT JOIN ", train_table, " ON "),
    id_string = str_c("train_base.case_id = ",train_table, ".case_id"),
    join_id_string = str_c(join_string, id_string)
  ) %>% unique()

# create query for joining relevant features to train_test
test_join <- revised_features_tbl %>% filter(str_detect(features, "case_id", negate = TRUE)) %>% na.omit() %>% 
  # table.column query
  mutate(
  table_column = str_c(test_table,".", features)) %>% 
  select(test_table, table_column) %>% 
  mutate(
    join_string = str_c("LEFT JOIN ", test_table, " ON "),
    id_string = str_c("test_base.case_id = ",test_table, ".case_id"),
    join_id_string = str_c(join_string, id_string)
  ) %>% unique()
    ```
   - `train_base` set up
    ```{r}
      train_join %>% select(table_column, join_id_string) %>% head()
    ```
   - `test_base` set up
    ```{r}
      test_join %>% select(table_column, join_id_string) %>% head()
    ```

* The parts can be strung together for the final query.
    ```{r}
# create the final train join query

# all join TABLE on TABLE
train_join_list <- train_join$join_id_string %>% unique() %>%  str_c(collapse = " ")
# all TABLE.COLUMNS
train_table_column_join <- train_join$table_column %>% str_c(collapse = ", ") 
# full query
train_join_query <- train_table_column_join   %>% 
  # select train_base columns 
  str_c("SELECT train_base.*, ",.) %>% 
  # 
  str_c(., " FROM train_base") %>% 
  # left join table statements
  str_c(., " ",train_join_list) %>% 
  # save as new table
  str_c("CREATE TABLE IF NOT EXISTS train_joined AS ",.)

# create the final train join query

# all join TABLE on TABLE
test_join_list <- test_join$join_id_string %>% unique() %>%  str_c(collapse = " ")
# all TABLE.COLUMNS
test_table_column_join <- test_join$table_column %>% str_c(collapse = ", ") 
# full query
test_join_query <- test_table_column_join   %>% 
  # select test_base columns 
  str_c("SELECT test_base.*, ",.) %>% 
  # 
  str_c(., " FROM test_base") %>% 
  # left join table statements
  str_c(., " ",test_join_list) %>% 
  # save as new table
  str_c("CREATE TABLE IF NOT EXISTS test_joined AS ",.)
    ```
    - `train_base` join query
    ```{r}
    train_join_query 
    ```
    - `test_base` join query
    ```{r}
    test_join_query
    ```


```{r, eval = FALSE}
# join tables with selected features
dbExecute(con, test_join_query)
dbExecute(con, train_join_query)
```

## Data analysis

Now that we have a dataset with all our relevant features, we can move into data anaysis and prediction. First we need to define our variable types across our test and train data. 

```{r,eval = FALSE, warning = FALSE}
# read train table
train_joined_df <- dbGetQuery(con, "SELECT * FROM train_joined") %>% tibble()
"train_joined_df" %>% quickrds()
# read test table
test_joined_df <- dbGetQuery(con, "SELECT * FROM test_joined") %>% tibble()
"test_joined_df" %>% quickrds()
```

* Import and define variable types. 
    - `
    - `categorical` variables (e.g., )
    - `numeric`: 
    ```{r,warning = FALSE, class.source = 'fold_show'}
train_joined_df<- read_rds("train_joined_df.rds")
# define train table variables
train_joined_tbl <- train_joined_df %>%   
  # convert date to numeric
  mutate(
  date_decision = as_date(date_decision),
  birth_259D = as_date(birth_259D),
  age = time_length(difftime(date_decision, birth_259D), "years"), 
  month.n = month(date_decision),
  year.n = year(date_decision),
  week.n = WEEK_NUM %>% as.numeric())  %>%
  # label numeric, factors
  mutate(
    across(c(matches("mainoccupationinc_384A|riskassesment_940T|
                     age|\\.n")), 
           ~ as.numeric(.)),
    across(c(matches("education_927M|empl_employedtotal_800L|empl_industry_691L|
                     familystate_447L|incometype_1044T|target|familystate_447L")), 
           ~ as.factor(.))) 
"train_joined_tbl" %>% quickrds()
    ```
    ```{r,warning = FALSE}
test_joined_df<- read_rds("test_joined_df.rds")

# define test table variables
test_joined_tbl <- test_joined_df %>%   
  # convert date to numeric
  mutate(
  date_decision = as_date(date_decision),
  birth_259D = as_date(birth_259D),
  age = time_length(difftime(date_decision, birth_259D), "years"),
  month.n = month(date_decision),
  year.n = year(date_decision),
  week.n = WEEK_NUM %>% as.numeric()) %>%
  # label numeric, factors
  mutate(
    across(c(matches("mainoccupationinc_384A|riskassesment_940T|
                     age|\\.n")), 
           ~ as.numeric(.)),
    across(c(matches("education_927M|empl_employedtotal_800L|empl_industry_691L|
                     familystate_447L|incometype_1044T|target|familystate_447L")), 
           ~ as.factor(.))) 
"test_joined_tbl" %>% quickrds()
    ```
    - `train` df
    ```{r}
    train_joined_tbl %>% glimpse()
    ```
    - `test` df
    ```{r}
    test_joined_tbl %>% glimpse()
    ```

### Summary statistics

Here's a look at some summary statistics. 

### Nominal variables

```{r}
library(summarytools)

# factor level proportions
train_joined_tbl %>% select(where(is.factor)) %>% freq()
```

### Numeric variables

```{r}
# numeric variable summaries
train_joined_tbl %>% select(where(is.numeric)) %>% descr()
```

### Summary

We see that `riskassesment_940T` has `r (mean(is.na(train_joined_tbl$riskassesment_940T))*100) %>% round(.,1) `% missing entries so it may be best to leave it out for now. 






